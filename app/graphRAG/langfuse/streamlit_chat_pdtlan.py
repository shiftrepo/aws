import os
import uuid
import logging
import streamlit as st
import boto3
import tenacity
import textwrap
import urllib3
import requests
from botocore.config import Config

# LlamaIndex Core imports
from llama_index.core import PropertyGraphIndex, Settings
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.llms.bedrock_converse import BedrockConverse
from llama_index.embeddings.bedrock import BedrockEmbedding

# Langfuseé–¢é€£
from llama_index.core.callbacks import CallbackManager
from langfuse.llama_index import LlamaIndexCallbackHandler, LlamaIndexInstrumentor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸŒ Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())[:8]
session_id = st.session_state["session_id"]
logger.info(f"Streamlit Session ID: {session_id}")

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã®ç¢ºèªã¨é©ç”¨
http_proxy = os.getenv("http_proxy")
https_proxy = os.getenv("https_proxy") or http_proxy  # https_proxyãŒãªã‘ã‚Œã°http_proxyã‚’ä½¿ç”¨

if http_proxy:
    logger.info(f"ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’æ¤œå‡ºã—ã¾ã—ãŸ: {http_proxy}")
    # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç’°å¢ƒã«é©ç”¨
    os.environ["HTTP_PROXY"] = http_proxy
    os.environ["HTTPS_PROXY"] = https_proxy
    
    # requestsã¨urllib3ã«ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’é©ç”¨
    proxies = {
        "http": http_proxy,
        "https": https_proxy
    }
    urllib3.util.connection.create_connection = lambda *args, **kwargs: urllib3.util.connection._create_connection(*args, **kwargs)
    
    # AWS SDKç”¨ã®ãƒ—ãƒ­ã‚­ã‚·è¨­å®š
    boto3_config = Config(
        proxies={
            'http': http_proxy,
            'https': https_proxy
        }
    )
    logger.info("ãƒ—ãƒ­ã‚­ã‚·è¨­å®šãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ")
else:
    logger.info("ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã¯ã‚ã‚Šã¾ã›ã‚“")
    boto3_config = Config()

# ğŸ”§ ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šUI
with st.sidebar:
    st.header("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š")
    
    # LLMãƒ¢ãƒ‡ãƒ«é¸æŠ
    st.subheader("LLMãƒ¢ãƒ‡ãƒ«")
    llm_options = {
        "Anthropic Claude 3.5 Haiku": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
        "Anthropic Claude 3.5 Sonnet v2": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "Anthropic Claude 3.7 Sonnet": "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
    }
    
    selected_llm_name = st.selectbox(
        "ä½¿ç”¨ã™ã‚‹LLMã‚’é¸æŠ",
        options=list(llm_options.keys()),
        index=0
    )
    
    selected_model_id = llm_options[selected_llm_name]
    st.info(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {selected_model_id}")
    
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
    max_output_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°", min_value=100, max_value=4096, value=1024, step=100)
    max_text_length = st.slider("æœ€å¤§ãƒ†ã‚­ã‚¹ãƒˆé•·ã• (æ–‡å­—æ•°)", min_value=1000, max_value=10000, value=3000, step=100)
    
    # Neo4j RAGè¨­å®š
    st.subheader("Neo4j RAGè¨­å®š")
    use_neo4j_rag = st.toggle("Neo4j RAGã‚’ä½¿ç”¨ã™ã‚‹", value=False)
    
    if use_neo4j_rag:
        neo4j_username = st.text_input("Neo4j ãƒ¦ãƒ¼ã‚¶ãƒ¼å", value=os.getenv("NEO4J_USERNAME", "neo4j"))
        neo4j_password = st.text_input("Neo4j ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", value=os.getenv("NEO4J_PASSWORD", "password"), type="password")
        neo4j_url = st.text_input("Neo4j URL", value=os.getenv("NEO4J_URL", "bolt://neo4jRAG:7687"))
    
    # Langfuseè¨­å®š
    st.subheader("Langfuse ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®š")
    use_langfuse = st.toggle("Langfuseãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=False)
    
    if use_langfuse:
        langfuse_public_key = st.text_input("Langfuse Public Key", value=os.getenv("LANGFUSE_PUBLIC_KEY", ""))
        langfuse_secret_key = st.text_input("Langfuse Secret Key", value=os.getenv("LANGFUSE_SECRET_KEY", ""), type="password")
        langfuse_host = st.text_input("Langfuse Host", value=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com"))

# AWSèªè¨¼
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
aws_region = os.getenv('AWS_DEFAULT_REGION')

if not (aws_access_key_id and aws_secret_access_key and aws_region):
    st.error("AWSã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    st.stop()

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’é©ç”¨ã—ãŸAWSæ¥ç¶š
boto3_session = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
bedrock_client = boto3_session.client('bedrock-runtime', config=boto3_config)

# LLMï¼ˆé¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
llm = BedrockConverse(
    model=selected_model_id,
    client=bedrock_client,
    temperature=temperature,
    max_tokens=max_output_tokens
)

# Embedding - ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
@tenacity.retry(stop=tenacity.stop_after_attempt(3),
                wait=tenacity.wait_fixed(5),
                retry=tenacity.retry_if_exception_type(Exception),
                before_sleep=tenacity.before_sleep_log(logging, logging.WARNING))
def create_embedding_with_retry(client, model_name, request_timeout):
    logger.info("Embedding ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­...")
    return BedrockEmbedding(
        model_name=model_name,
        client=client,
        use_async=False,
        request_timeout=request_timeout
    )

embedding = create_embedding_with_retry(
    bedrock_client,
    "amazon.titan-embed-text-v2:0",
    request_timeout=60
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
Settings.llm = llm
Settings.embed_model = embedding

# LangfuseåˆæœŸåŒ–ï¼ˆä½¿ç”¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
if use_langfuse and langfuse_public_key and langfuse_secret_key and langfuse_host:
    try:
        logger.info("Langfuseã®åˆæœŸåŒ–ã‚’é–‹å§‹ã—ã¾ã™")
        # Langfuse Instrumentor åˆæœŸåŒ–
        instrumentor = LlamaIndexInstrumentor(
            secret_key=langfuse_secret_key,
            public_key=langfuse_public_key,
            host=langfuse_host
        )
        
        # Langfuse Callback Handler è¨­å®š
        langfuse_callback_handler = LlamaIndexCallbackHandler(
            public_key=langfuse_public_key,
            secret_key=langfuse_secret_key,
            host=langfuse_host,
            tags=["streamlit-app", "neo4j-rag", f"st_session:{session_id}"]
        )
        logger.info("Langfuse callback handler ã‚’è¨­å®š")
        Settings.callback_manager = CallbackManager([langfuse_callback_handler])
        langfuse_callback_handler.set_trace_params(session_id=session_id)
    except Exception as e:
        logger.error(f"Langfuse callback handler ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        st.sidebar.error(f"Langfuseã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
else:
    logger.info("Langfuseã¯ä½¿ç”¨ã—ã¾ã›ã‚“")
    instrumentor = None

# Neo4j RAGã®åˆæœŸåŒ–ã¨è¨­å®šï¼ˆä½¿ç”¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
if use_neo4j_rag:
    try:
        logger.info("Neo4j RAGã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™")
        graph_store = Neo4jPropertyGraphStore(
            username=neo4j_username,
            password=neo4j_password,
            url=neo4j_url,
        )
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
        try:
            index = PropertyGraphIndex.from_existing(
                property_graph_store=graph_store,
                show_progress=True,
            )
            
            # Retriever + Chat Engine
            if "chat_engine" not in st.session_state or "last_selected_model" not in st.session_state or st.session_state["last_selected_model"] != selected_model_id:
                retriever = index.as_retriever(include_text=False)
                memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
                chat_engine = ContextChatEngine.from_defaults(
                    retriever=retriever,
                    memory=memory,
                )
                st.session_state["chat_engine"] = chat_engine
                st.session_state["last_selected_model"] = selected_model_id
                st.session_state["using_neo4j"] = True
            
            chat_engine = st.session_state["chat_engine"]
            logger.info("Neo4j RAGãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            st.error(f"æ—¢å­˜ã®ã‚°ãƒ©ãƒ•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            use_neo4j_rag = False
    except Exception as e:
        logger.error(f"Neo4jæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"Neo4jã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        use_neo4j_rag = False
else:
    # RAGã‚’ä½¿ç”¨ã—ãªã„å ´åˆã¯ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
    if "chat_engine" not in st.session_state or st.session_state.get("using_neo4j", False) or "last_selected_model" not in st.session_state or st.session_state["last_selected_model"] != selected_model_id:
        # é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
        memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
        from llama_index.core.llms import ChatMessage, MessageRole
        from llama_index.core.chat_engine.simple import SimpleChatEngine
        
        chat_engine = SimpleChatEngine.from_defaults(
            llm=llm,
            memory=memory,
        )
        st.session_state["chat_engine"] = chat_engine
        st.session_state["last_selected_model"] = selected_model_id
        st.session_state["using_neo4j"] = False
    
    chat_engine = st.session_state["chat_engine"]
    logger.info("é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™")

# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹é–¢æ•°
def split_text(text, max_length):
    # textwrapã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    return textwrap.wrap(text, width=max_length)

# Streamlit UI
st.title("LLM ãƒãƒ£ãƒƒãƒˆ" + (" + Neo4j RAG" if use_neo4j_rag else ""))
st.caption(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«: {selected_llm_name}" + (", Langfuseãƒˆãƒ¬ãƒ¼ã‚¹æœ‰åŠ¹" if use_langfuse else ""))

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_files = st.file_uploader("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["txt"], accept_multiple_files=True)

# ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å¤‰æ•°åˆæœŸåŒ–
split_content = []

# ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
if uploaded_files:
    all_content = ""
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        all_content += f"\n\n----- {uploaded_file.name} -----\n\n{file_content}"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒè¨­å®šã•ã‚ŒãŸæœ€å¤§æ–‡å­—æ•°ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿åˆ†å‰²
    if len(all_content) > max_text_length:
        split_content = split_text(all_content, max_text_length)
    else:
        split_content = [all_content]  # åˆ†å‰²ã›ãšãã®ã¾ã¾ä½¿ç”¨
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«åˆ†å‰²ã•ã‚ŒãŸå†…å®¹ã‚’è¡¨ç¤º
    st.text_area("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹", value="\n".join(split_content), height=300)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

for role, content in st.session_state["chat_history"]:
    with st.chat_message(role):
        st.markdown(content)

# å…¥åŠ›å‡¦ç†
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    logger.info(f"User input: {prompt}")
    st.session_state["chat_history"].append(("user", prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¨è³ªå•ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    if split_content:
        full_prompt = f"""æ¬¡ã®è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„:

{textwrap.shorten("\n".join(split_content), width=max_text_length)}

è³ªå•: {prompt}
"""
    else:
        full_prompt = prompt
    
    # LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("è€ƒãˆä¸­..."):
            status_placeholder = st.empty()  # å‹•çš„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨
            try:
                if use_neo4j_rag:
                    status_placeholder.info("Retriever ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™...")
                else:
                    status_placeholder.info("å¿œç­”ã‚’ç”Ÿæˆä¸­...")
                
                response = chat_engine.chat(full_prompt)

                status_placeholder.info("å¿œç­”ã‚’ç”Ÿæˆä¸­...")
                response_text = response.response
                logger.info(f"Response: {response_text}")

                status_placeholder.empty()
                st.markdown(response_text)
                st.session_state["chat_history"].append(("assistant", response_text))
                
                # Langfuseãƒˆãƒ¬ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã¯ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                if use_langfuse and instrumentor:
                    instrumentor.flush()

            except Exception as e:
                logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
                status_placeholder.error("å¿œç­”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                st.error(f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.session_state["chat_history"].append(("assistant", f"ã‚¨ãƒ©ãƒ¼: {e}"))
