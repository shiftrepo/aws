import os
import uuid
import logging
import streamlit as st
import boto3
import tenacity
import textwrap

# LlamaIndex Core imports
from llama_index.core import PropertyGraphIndex, Settings
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.llms.bedrock_converse import BedrockConverse
from llama_index.embeddings.bedrock import BedrockEmbedding

# Langfuseé–¢é€£
from llama_index.core.callbacks import CallbackManager
from langfuse.llama_index import LlamaIndexCallbackHandler, LlamaIndexInstrumentor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸŒ Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())[:8]
session_id = st.session_state["session_id"]
logger.info(f"Streamlit Session ID: {session_id}")

# ğŸ”§ LLM è¨­å®š UIï¼ˆè¿½åŠ ï¼‰
with st.sidebar:
    st.header("LLMè¨­å®š")
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
    max_output_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°", min_value=100, max_value=4096, value=1024, step=100)
    max_text_length = st.slider("æœ€å¤§ãƒ†ã‚­ã‚¹ãƒˆé•·ã• (æ–‡å­—æ•°)", min_value=1000, max_value=10000, value=3000, step=100)  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨­å®šã§ãã‚‹æœ€å¤§æ–‡å­—æ•°

# Langfuseç’°å¢ƒå¤‰æ•°
langfuse_public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
langfuse_secret_key = os.getenv("LANGFUSE_SECRET_KEY")
langfuse_host = os.getenv("LANGFUSE_HOST")

# Langfuse Instrumentor åˆæœŸåŒ–
instrumentor = LlamaIndexInstrumentor(
    secret_key=langfuse_secret_key,
    public_key=langfuse_public_key,
    host=langfuse_host
)

# Langfuse Callback Handler è¨­å®š
if not (langfuse_public_key and langfuse_secret_key and langfuse_host):
    logger.warning("Langfuseã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
else:
    try:
        langfuse_callback_handler = LlamaIndexCallbackHandler(
            public_key=langfuse_public_key,
            secret_key=langfuse_secret_key,
            host=langfuse_host,
            tags=["streamlit-app", "neo4j-rag", f"st_session:{session_id}"]
        )
        logger.info("Langfuse callback handler ã‚’è¨­å®š")
        Settings.callback_manager = CallbackManager([langfuse_callback_handler])
        langfuse_callback_handler.set_trace_params(session_id=session_id)
    except Exception as e:
        logger.error(f"Langfuse callback handler ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")

# AWSèªè¨¼
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
aws_region = os.getenv('AWS_DEFAULT_REGION')

if not (aws_access_key_id and aws_secret_access_key and aws_region):
    st.error("AWSã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    st.stop()

boto3_session = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
bedrock_client = boto3_session.client('bedrock-runtime')

# LLMï¼ˆå¤‰æ›´ã‚ã‚Šï¼štemperatureã¨max_tokensè¿½åŠ ï¼‰
llm = BedrockConverse(
    model="anthropic.claude-3-haiku-20240307-v1:0",
    client=bedrock_client,
    temperature=temperature,
    max_tokens=max_output_tokens
)

# Embedding
@tenacity.retry(stop=tenacity.stop_after_attempt(3),
                wait=tenacity.wait_fixed(5),
                retry=tenacity.retry_if_exception_type(Exception),
                before_sleep=tenacity.before_sleep_log(logging, logging.WARNING))
def create_embedding_with_retry(client, model_name, request_timeout):
    logger.info("Embedding ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­...")
    return BedrockEmbedding(
        model_name=model_name,
        client=client,
        use_async=False,
        request_timeout=request_timeout
    )

embedding = create_embedding_with_retry(
    bedrock_client,
    "amazon.titan-embed-text-v2:0",
    request_timeout=60
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
Settings.llm = llm
Settings.embed_model = embedding

# Neo4j æ¥ç¶š
neo4j_username = os.getenv("NEO4J_USERNAME", "neo4j")
neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
neo4j_url = os.getenv("NEO4J_URL", "bolt://neo4jRAG:7687")

graph_store = Neo4jPropertyGraphStore(
    username=neo4j_username,
    password=neo4j_password,
    url=neo4j_url,
)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
try:
    index = PropertyGraphIndex.from_existing(
        property_graph_store=graph_store,
        show_progress=True,
    )
except Exception as e:
    logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    st.error(f"æ—¢å­˜ã®ã‚°ãƒ©ãƒ•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

# Retriever + Chat Engine
if "chat_engine" not in st.session_state:
    retriever = index.as_retriever(include_text=False)
    memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
    chat_engine = ContextChatEngine.from_defaults(
        retriever=retriever,
        memory=memory,
    )
    st.session_state["chat_engine"] = chat_engine

chat_engine = st.session_state["chat_engine"]

# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹é–¢æ•°
def split_text(text, max_length):
    # textwrapã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    return textwrap.wrap(text, width=max_length)

# Streamlit UI
st.title("Neo4j + Claude Haiku ãƒãƒ£ãƒƒãƒˆ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_files = st.file_uploader("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["txt"], accept_multiple_files=True)

# ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
if uploaded_files:
    all_content = ""
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
    for uploaded_file in uploaded_files:
        file_content = uploaded_file.read().decode("utf-8")
        all_content += f"\n\n----- {uploaded_file.name} -----\n\n{file_content}"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒè¨­å®šã•ã‚ŒãŸæœ€å¤§æ–‡å­—æ•°ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿åˆ†å‰²
    if len(all_content) > max_text_length:
        split_content = split_text(all_content, max_text_length)
    else:
        split_content = [all_content]  # åˆ†å‰²ã›ãšãã®ã¾ã¾ä½¿ç”¨
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«åˆ†å‰²ã•ã‚ŒãŸå†…å®¹ã‚’è¡¨ç¤º
    st.text_area("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹", value="\n".join(split_content), height=300)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

for role, content in st.session_state["chat_history"]:
    with st.chat_message(role):
        st.markdown(content)

# å…¥åŠ›å‡¦ç†
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    logger.info(f"User input: {prompt}")
    st.session_state["chat_history"].append(("user", prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¨è³ªå•ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    full_prompt = f"""æ¬¡ã®è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„:

{textwrap.shorten("\n".join(split_content), width=max_text_length)}

è³ªå•: {prompt}
"""
    
    # LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("è€ƒãˆä¸­..."):
            status_placeholder = st.empty()  # å‹•çš„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨
            try:
                status_placeholder.info("Retriever ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™...")
                response = chat_engine.chat(full_prompt)

                status_placeholder.info("å¿œç­”ã‚’ç”Ÿæˆä¸­...")
                response_text = response.response
                logger.info(f"Response: {response_text}")

                status_placeholder.empty()
                st.markdown(response_text)
                st.session_state["chat_history"].append(("assistant", response_text))
                instrumentor.flush()

            except Exception as e:
                logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
                status_placeholder.error("å¿œç­”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                st.error(f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.session_state["chat_history"].append(("assistant", f"ã‚¨ãƒ©ãƒ¼: {e}"))

